
# ğŸ§  RAG Chatbot (Streamlit + ChromaDB + Groq)

This project is a **Retrieval-Augmented Generation (RAG)** chatbot built with **Streamlit**, **LangChain**, **ChromaDB**, and **Groq**. It enables users to query a pre-loaded document (e.g., a PDF) and receive intelligent answers powered by Groqâ€™s LLM. Users **do not upload** their own documentsâ€”the document is already processed and indexed in the backend.

---

## ğŸš€ Key Features (MVPs)

- âœ… Load and split a static PDF into text chunks
- âœ… Generate sentence embeddings using SentenceTransformer (`all-MiniLM-L6-v2`)
- âœ… Store and retrieve document vectors from **ChromaDB**
- âœ… Embed user queries and match them semantically with chunks
- âœ… Use **Groq LLM** (via LangChain) to respond to questions with retrieved context
- âœ… Real-time interactive **Streamlit chatbot UI**

---

## ğŸ§  High-Level System Architecture

```plaintext
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  .env File  â”‚        â—„â”€â”€ GROQ API Key, Model Name
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PDF Loader  â”‚        â—„â”€â”€ PDF is loaded from /data/
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Text Splitter    â”‚        â—„â”€â”€ RecursiveCharacterTextSplitter
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Embedding Model  â”‚        â—„â”€â”€ SentenceTransformer (MiniLM)
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ChromaDB        â”‚        â—„â”€â”€ Vector Store (document chunks)
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit Chatbot UI   â”‚  â—„â”€â”€ Groq LLM fetches response based on query + context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ chatbot_main.py         # Streamlit app: chatbot interface + Groq LLM
â”œâ”€â”€ chroma_setup.py         # Handles loading PDF, chunking, embedding, and storing in ChromaDB
â”œâ”€â”€ data/
â”‚   â””â”€â”€ the-seven-habits.pdf  # Preloaded PDF document
â”œâ”€â”€ data_store/             # Persistent ChromaDB vector database
â”œâ”€â”€ Dockerfile              # Docker config for Streamlit app
â”œâ”€â”€ docker-compose.yml      # Orchestrates Streamlit app + ChromaDB
â”œâ”€â”€ .env                    # API credentials (GROQ_API_KEY and GROQ_MODEL)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ³ Docker Setup

### 1. Docker Compose File

```yaml
version: "3.9"
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    env_file:
      - .env

  chroma:
    image: chromadb/chroma:latest
    volumes:
      - ./chroma_db:/data
    ports:
      - "8765:8000"
```

---

### 2. Dockerfile

```Dockerfile
# Streamlit app Dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY . .

RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 8501
CMD ["streamlit", "run", "chatbot_main.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

## âœ… How It Works

1. The backend loads a static PDF (`the-seven-habits.pdf`)
2. The text is split into smaller chunks
3. Each chunk is embedded into a vector using a sentence transformer model
4. The vectors are stored in ChromaDB
5. When a user sends a query:
   - The query is embedded
   - A similarity search retrieves relevant chunks
   - The Groq LLM is prompted with both the query and the relevant context

---

## ğŸ’¡ Technologies Used

- **Streamlit** â€” UI for chat interactions
- **LangChain** â€” Framework for chaining LLM calls and managing messages
- **ChromaDB** â€” Vector store to store and retrieve document embeddings
- **SentenceTransformers** â€” Local embedding model (`all-MiniLM-L6-v2`)
- **Groq LLM (Llama 3)** â€” Used via LangChainâ€™s `ChatGroq`

---

## ğŸŒ± Future Improvements

- Allow document uploads by users (optional)
- Add citation sources from PDF in each answer
- Add feedback mechanism to rate answers
- Add PDF summarization feature
- Host on a public cloud (e.g., Hugging Face Spaces or Render)

---

## ğŸ Getting Started (Local)

```bash
git clone https://github.com/your-username/module-2.git
cd module-2

# Set up virtual environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
echo "GROQ_API_KEY=your_key" > .env
echo "GROQ_MODEL=llama-3-8b" >> .env

# Run with Streamlit
streamlit run chatbot_main.py
```

---

## âœï¸ Author

Built by [Your Byaruhanga Johnson]

